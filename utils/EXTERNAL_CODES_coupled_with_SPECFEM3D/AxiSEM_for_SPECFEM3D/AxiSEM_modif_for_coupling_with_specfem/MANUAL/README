                              AXISEM 
   A parallel 2-D spectral-element solver for 3-D elastodynamics in 
           global, spherically symmetric background models.

(c) 2008 Tarje Nissen-Meyer, tarje@alumni.princeton.edu, Princeton Univerity

Co-developers: Alexandre Fournier, F. A. Dahlen

-----------------------------------------------------------------------------

Includes:
- Global numbering scheme based on P. Fischer & H. Tufo
- Symplectic time integration scheme (co-developed with Jean-Paul Ampuero)
- Output for multiple-frequency full-waveform sensitivity kernels 

Program language: F90, MPI
Main development software: ifort, mpich

Acknowledgments: :::::::::::::::::::::::::::::::::::::::::::::::::::::::::
Jeroen Tromp, Dimitri Komatitsch, Guust Nolet, Emanuel Chaljub

References::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::

(1) Tarje Nissen-Meyer, F. A. Dahlen, A Fournier (2007) 
"Spherical-earth Frechet sensitivity kernels"        
Geophysical Journal International 168(3),1051-1066. 
doi:10.1111/j.1365-246X.2006.03123.x                
                                                        
(2) Tarje Nissen-Meyer, A Fournier, F. A. Dahlen (2007)   
"A two-dimensional spectral-element method for
spherical-earth seismograms-I. Moment-tensor source" 
Geophysical Journal International 168(3), 1067-1092. 
doi:10.1111/j.1365-246X.2006.03121.x                 
                                                       
(3) Tarje Nissen-Meyer, A Fournier, F. A. Dahlen (2008)  
"A two-dimensional spectral-element method for   
spherical-earth seismograms - II. Waves in solid-fluid media" 
Geophysical Journal International, 174(3), 873-888.
doi:10.1111/j.1365-246X.2008.03813.x

(4) Tarje Nissen-Meyer (2007)
"Full-wave seismic sensitivity in a spherical Earth"
Ph.D. thesis, Princeton University
(This includes refs (1)-(3) and more details.)

(5) Jean-Paul Ampuero, Tarje Nissen-Meyer (2008)
"High-order conservative time schemes in spectral-element methods 
for seismic wave propagation."
To be submitted to Geophysical Journal International.

-----------------------------------------------------------------------------
This software is subdivided into two components: 
Mesher and Solver (located in respective directories).

Most relevant routines are documented, here we just give a brief 
overview over the code structure.

General remarks on both codes:
- All variables and constants that are needed across modules are defined.
in files named data_*.f90. 
- Global constants are defined in global_parameters.f90
- Changes in any of the input files do NOT require recompilation, changes in 
any *.f90 file DO require recompilation. 
- Any new mesh requires recompilation of the solver.
- Standard output is written to files called 
OUTPUT (mesher) and OUTPUT_<dir> (solver), 
and contains all relevant information regarding either run.

=============================================================================
MESHER: Constructs the spherical 2-D mesh of spectral elements based 
on a given background model, a target resolution as defined by 
the (dominant) source period, the number of processors available, 
and the polynomail order of the spectral elements. 

This mesher is NOT well documented or cleanly coded, and subject to 
streamlining efforts. Please let us know if you intend to add comments or 
change code to join forces.
It invokes a series of dingy, multiply layered bookkeeping index arrays that 
are not easy to follow (see parallelization.f90 pdb.f90). 
Again, we welcome improvements here but want to caution users to change 
anything if not entirely sure about all dependencies.

NOTE: This mesher is serial but constructs the parallel mesh database 
needed in the solver (in pdb.f90), easily down to periods of about 
3 seconds on typical memory chips (~2GB). 
For higher resolutions, reordering of global variables will be done (TODO).

COMPILATION: Check Makefile to be sure the right compilers and flags are used.
Use 'makemake.pl gllmesh' to recreate a Makefile when adding routines etc.
The executable is called gllmesh. 
Compile via 'make clean; make'

RUN MESHER: For a queuing system based on Torque/Maui, check the file 
run_mesher for the correct path, and then submit via:
qsub run_mesher

RUN TIME, MEMORY OCCUPATION, NUMBER OF PROCESSORS:
The simulations should be done within minutes for meshes above 10 seconds.
Note that doubling the resolution (half the period) results in a mesh that 
is 4 times larger and has about half the time step, i.e. the solver takes 
double the time at 4 times more processors. Increasing the number of 
processors keeping resolution constant will slow the mesher insignificantly, 
but in most cases speed up the solver substantially, but not linearly
(the more message passing, the slower).
This mesher is completely serial but constructs the parallel mesh database 
needed in the solver (in pdb.f90), easily down to periods of about 
3 seconds on typical memory chips (~2GB). 

INPUT FILES: inparam_mesh, inparam_model

inparam_model: defines all relevant parameters, self-explanatory
Some more comments:
- background models: be sure that the string defined here exists in 
background_models.f90 
- Keep this as true unless you know otherwise
- number of elements per wavelength: This will be used to compute the largest
allowable grid spacing based on the dominant source period and is intimately 
tied to the polynomial order npol (defined in data_spec.f90). See ref (5) for 
a comprehensive analysis on how to choose the parameters if necessary.
If the seismograms contain unreasonable noise (usually high-frequency tails), 
are inaccurate or otherwise different than a reference solution or data, 
try to increase this value. This will result in a denser mesh at higher 
simulation cost (possibly more processors), but deliver more accurate results. 
The absolute minimum number of grid points per maximum wavelength for S waves 
is about 4.5 (i.e., this parameter times npol). See ref (3) for details
- dominant source period: This is the peak spectrum of the source time 
function (STF). In most applications, the solver should be run using a Dirac d
elta function and then convolved a posteriori with the STF. In other words, 
the mesh is constructed such that the solver will compute Green functions 
that are accurate up to this resolution.
- Courant number: Defines the stability criterion to choose the time step.
0.6 is the heuristically determined maximal value for any realistic 
applications. If the solver explodes, try to decrease this number.
- number of processors: needs to be a multiple of 2. 
- coarsening levels: Keep this at 3 if applied to any typical earth models. 
if the total global variation in wave speed is significantly different, 
other options may be convenient here. 

inparam_mesh: to be purged, do not touch unless you are exactly aware 
of the consequences

Code structure: see main.f90 for the main components.

BACKGROUND MODELS: need to add TWO routines to 
background_models.f90, one for the discontinuities, one for the 
actual media parameters. See the module for details. Note that this module 
will be used in the solver too.

DOMAIN DECOMPOSITION: in parallelization.f90 
We assume a simple colatitudinal cake-piece decomposition for the outer 
spherical part of the mesh. The central, rectangular part is trickier. 
We employ a method that is based on the following principles:
- exact load balancing 
- maximally 2 neighboring domains, i.e. each domain touches the axis
- at least two elements thickness
- spherical cake-piece decomposition as 'boundary condition'
Upon this, we construct the decomposition by approximating polynomials s(z)
of varying degrees. For the time being, this is only implemented up to 
16 processors, but no hard limitation. See the dissertation,ref(4) for details.

OUTPUT: 
NEEDED BY SOLVER: mesh_params.h and meshdb.dat0000,...,meshdb.dat00<nproc-1>, 
unrolled_loops.f90, background_models.f90

NOT NEEDED BY SOLVER: Everything else including various meshes and global 
arrays such as valence to be plotted using complementary shell scripts

mesh_params.h: This is a header file that contains all the static array 
sizes for the solver such that it does not need to allocate (a lot of) 
memory dynamically. Specifically, it documents all revelant mesh input 
parameters commented at the top, and some output information such as 
time step at the bottom.  The inclusion of this header into the solver 
requires recompilation for each new mesh. 

meshdb.dat00*: These are the complete databases for each processor of the 
solver simulations. They contain the skeleton mesh, regions, boundary 
information (axis, free surface, solid-fluid boundary, processor boundary), 
and basically everything needed for a full simulation except for the 
source-receiver specification which is done inside the solver. 

After these two crucial outputs are written, the mesher reloads them as a
simple test on some of their dependencies. More comprehensive tests are 
performed in the solver. 

PREPARE FILES TO BE USED IN SOLVER:
Use the script "copymesh" to transfer all relevant files to the SOLVER 
directory (or a "MESHES" directory therein). It may be useful to simply 
produce a series of meshes for a given background model and varying 
resolution and numbers of processors. 

=============================================================================
SOLVER: Picks up the header from the mesher into its compilation to solve 
the elastodynamic 3-D equations of motion for spherically symmetric 
earth models in a 2-D computational domain using message-passing for the 
parallelization. The full seismic moment-tensor is accounted for by 
separate simulations: two monopole, one dipole, and one quadrupole 
simulation have to be done to sum up to the 6 indepedent elements of Mij.
The software has mainly been developed for the purpose of computing and 
saving entire wavefields that form the basis of full-wave based, 
multiple-frequency sensitivity kernels. This output is included in the 
solver, but the actual calcuation of kernels needs to be done elsewhere. 
The philosophy of this 'scattering-integral' approach to sensitivity kernels 
is a once-and-for-all calculation of a database for a given background model,
i.e. including all potential earthquake depths. Everything else related to 
data, measurements, inversion parameters and observables, time windows, etc 
is done at the stage of inversion. At this time, output is limited to 
saving snapshots of the entire 2D mesh. Under development is a more 
sophisticated method to either sample more sparsely or output into a wavelet 
basis. 
Of course, this method can simply be used to solve the forward problem and 
compute accurate seismograms for spherically symmetric background models up to
arbirtrary frequencies. 

Detailed information on the various components of the code are included as 
comments to the respective modules and routines. 
Here, we give a simple overview over the main parts. 

COMPILATION: Check Makefile for correct compiler and flags. If code is altered,
use 'makemake.pl f_specfem' to recreate new Makefile. 
Compile via 'make clean; make'

RUN SOLVER: Script 'submit.csh NAME_OF_RUN' is used to submit a job to a 
Torque/Maui queue. See the file for all necessary files to be copied into 
the new run directory.

INPUT FILES: 

- automatically taken from mesher (see mesher readme): 
mesh_params.h: header including array sizes necessary for the compilation
meshdb.dat00*: mesh databases for each processor
unrolled_loops.f90: unrolled loops and unit-stride cache access routines 
                    for given polynomial order
background_models.f90: all background models. Needs to be the same as in mesher
Any changes to background model, mesh, polynomial order etc require 
recompilation.

- inparam, sourceparams.dat, receivers.dat
These 3 solver input files may be changed without recompilation of the solver 
source code.

GENERAL INPUT: inparam, self-explanatory, a few extra comments:
- time step: should keep this at 0 unless the code blows up or one needs to 
have a specific sampling rate smaller than the value suggested by the mesher 
(found in mesh_params.h)
- period: should keep this at 0 unless one wishes to test accuracy for 
constant mesh at varying frequencies, or somehow the simulations show 
inaccuracies. This choice is ignored if a delta function is used as the source 
time function (in sourceparams.dat), which should be the preferred option
unless wavefield snapshots (for propagation movies etc) are needed. 
- time scheme: see time_evol_wave.f90. Newmark is the traditional 2nd-order 
scheme, symplec4 is a PEFRL scheme of 4th order, about 2.5 more CPU time 
required than the Newmark scheme but 40 times more accurate. See ref(5) for a
detailed analysis and rules of thumb for choosing the optimal time scheme. 
Generally, if the setup requires seismograms at more than 100 wavelengths 
in distance, we suggest using the symplectic scheme. The memory requirements 
for the symplectic scheme are in fact *lower* due to avoiding the acceleration
as a full wavefield.
- large test files: Suggested to avoid this for general runs, especially in 
the case of high resolution as these files become prohibitively large.
- energy: avoid this unless you are specifically interested in the kinetic,
potential and total energy of the solid, fluid and global domains since it 
involves a full evaluation of the stiffness matrix and is therefore quite 
CPU (and memory) intensive. 
- snaps: only if wavefield movies are of interest. CAUTION! These 
are huge files for high resolutions and should be avoided if not needed. 
- strain: this dumps the strain tensor elements and velocity wavefield for the
purpose of subsequently calculating waveform and banana-doughnut kernels.
CAUTION! Avoid this if you do not intend to calculate kernels, even more so 
than the snapshots, these are huge files.
- dumping method: see time_evol_wave.f90 for details. 
- Samples per period: Should be at least 2 to fulfill Nyquist, but more 
are highly recommended for the purpose of picking time windows when 
computing static kernels.
- homogeneous parameters: This is mainly for testing purposes and should be 
turned to false otherwise. Only possible for purely solid models. Basically 
takes a mesh for a complex model but populates the entire domain with this 
homogeneous media characterization.
- source vicinity: An analytical test on the accuracy of near-source 
radiation. Should only be on if this is relevant.

SOURCE: sourceparams.dat
Includes all parameters describing the earthquake (or other) source.
- magnitude: the code runs in SI units. We usually keep this at 10E20 to 
obtain somewhat realistic amplitudes. When actual CMT solutions are needed, 
one then sums the respective components multiplied by their Mij entry*10E-20.
- excitation type: earthquakes only contain mono-, di-, quadrupole radiation:
monopole: explosion, Mzz, Mxx+Myy, and vertical single force
dipole: Mxz,Myz, horizontal single forces
quadrupole: Mxy, Mxx-Myy
This is overdetermination, granted, but still a sensible check on whether 
one has the anticipated source defined - a VERY COMMON *source* of swiftly 
running wrong simulations! Consistency checks are included at run time. 
- NOTE: SOURCE DEPTH in KM, NOT radius!
- source time function: Dirac Delta (single forces)/Heaviside (moment tensor)
should be used in all cases except for wavefield snapshots or specific tests. 
This instantaneous source detonates at the first time step of the seismogram 
sampling rate (seis_it). REMEMBER this when loading seismograms!
Otherwise, various derivatives of a Gaussian are included, where the Gaussian
peak is at time 1.5*period.

As of now, the code only computes point sources, and simply takes the closest
grid point as the location. Mislocation errors are given in OUTPUT_<dir>

RECEIVERS: receivers.dat
This file contains a simple list of co-latitudes (or rather, 
great-circle distance from north pole). The first line denotes the number 
of receivers following. The code simply finds the closest grid point. 
Check OUTPUT_<dir> to see how mislocated they are.

AT RUNTIME: 'cd NAME_OF_RUN; ls'. You should see an executable, input and 
header files, a file called OUTPUT_NAME_OF_RUN, and files output_proc00*.dat,
as well as time stamps eventually (the pre-timeloop CPU time may take a few 
minutes). Time stamps are written every estimated 1% of the total runtime. 
Min/max displacement values are recorded and may be a good starting point in 
case the simulation blows up. 

CODE OVERVIEW: main.f90 being the wrapper routine, most of the relevant 
calls are from time_evol_wave.f90. The code contains ~200 potentially 
terminating if-statements which test a variety of generic issues prior 
to the time loop. 

TIME LOOP: Several time schemes are included, see descriptions there. 
All number crunching happens in unrolled_loops.f90.

OUTPUT SEISMOGRAMS: Default output are seismograms at the epicenter, hypocenter,
equator, and antipode. These are organized as (time,displacement component).
All other seismograms based on receivers.dat contain the displacement 
components only. Note the sampling rate being an input parameter 

OUTPUT ENERGY: Time series for total, kinetic, and potential energy of global,
solid and fluid domains.

OUTPUT WAVEFIELD SNAPSHOTS: Saves the displacement field on the entire mesh
at given intervals (see parameter in inparam for shot rate). If this option is
used, an according mesh is written to file too.

OUTPUT KERNEL WAVEFIELDS: Similar to snapshots, but the strain tensor needs 
to be computed on the fly first, see time_evol_wave.f90.

CPU TIME, MEMORY OCCUPATION: A run for dominant period of 9 seconds on 
4 processors and one-hour long seismograms should take no longer than 
5-8 hours. 



 

